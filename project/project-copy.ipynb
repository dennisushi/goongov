{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2744512",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÑ Loaded configuration from .env file\n",
      "‚úÖ Langfuse keys loaded\n",
      "‚úÖ Langfuse client initialized\n",
      "‚úÖ Holistic AI Bedrock helper function loaded\n",
      "\n",
      "üîë API Key Status:\n",
      "  ‚úÖ Holistic AI Bedrock credentials loaded (will use Bedrock)\n",
      "‚úÖ All imports successful!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import json\n",
    "from pathlib import Path\n",
    "from dotenv import load_dotenv\n",
    "import langfuse\n",
    "from langfuse import observe\n",
    "from langchain_core.tools import tool\n",
    "\n",
    "\n",
    "\n",
    "# ============================================\n",
    "# Load ENV (.env recommended)\n",
    "# ============================================\n",
    "env_path = Path('../.env')\n",
    "if env_path.exists():\n",
    "    load_dotenv(env_path)\n",
    "    print(\"üìÑ Loaded configuration from .env file\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  No .env file found - using environment variables or hardcoded keys\")\n",
    "\n",
    "# ============================================\n",
    "# üîë Langfuse Key Extraction\n",
    "# ============================================\n",
    "LANGFUSE_PUBLIC_KEY = \"pk-lf-43f9272f-d4a4-4efa-9a8c-d697f987f9b7\"\n",
    "LANGFUSE_SECRET_KEY = \"sk-lf-c95015fa-b35d-4885-b18c-cc10a21b6fba\"\n",
    "LANGFUSE_HOST = os.getenv(\"LANGFUSE_HOST\", \"https://cloud.langfuse.com\")\n",
    "\n",
    "if not LANGFUSE_PUBLIC_KEY or not LANGFUSE_SECRET_KEY:\n",
    "    print(\"‚ö†Ô∏è  Langfuse keys missing. Set LANGFUSE_PUBLIC_KEY and LANGFUSE_SECRET_KEY in .env.\")\n",
    "else:\n",
    "    print(\"‚úÖ Langfuse keys loaded\")\n",
    "\n",
    "# Initialize Langfuse client\n",
    "try:\n",
    "    langfuse_client = langfuse.Langfuse(\n",
    "        public_key=LANGFUSE_PUBLIC_KEY,\n",
    "        secret_key=LANGFUSE_SECRET_KEY,\n",
    "        host=LANGFUSE_HOST\n",
    "    )\n",
    "    print(\"‚úÖ Langfuse client initialized\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Failed to initialize Langfuse client: {e}\")\n",
    "\n",
    "# ============================================\n",
    "# Import Holistic AI Bedrock helper (Optional)\n",
    "# ============================================\n",
    "import sys\n",
    "try:\n",
    "    sys.path.insert(0, '../core')\n",
    "    from react_agent.holistic_ai_bedrock import HolisticAIBedrockChat, get_chat_model\n",
    "    print(\"‚úÖ Holistic AI Bedrock helper function loaded\")\n",
    "except ImportError:\n",
    "    print(\"‚ö†Ô∏è  Could not import from core - will use OpenAI only\")\n",
    "\n",
    "from langgraph.prebuilt import create_react_agent\n",
    "from langchain_core.messages import HumanMessage\n",
    "from pydantic import BaseModel, Field\n",
    "from typing import List\n",
    "\n",
    "# ============================================\n",
    "# Verify other API keys\n",
    "# ============================================\n",
    "print(\"\\nüîë API Key Status:\")\n",
    "if os.getenv('HOLISTIC_AI_TEAM_ID') and os.getenv('HOLISTIC_AI_API_TOKEN'):\n",
    "    print(\"  ‚úÖ Holistic AI Bedrock credentials loaded (will use Bedrock)\")\n",
    "elif os.getenv('OPENAI_API_KEY'):\n",
    "    print(\"  ‚ö†Ô∏è  OpenAI API key loaded (Bedrock credentials not set)\")\n",
    "    print(\"     üí° Tip: Set HOLISTIC_AI_TEAM_ID and HOLISTIC_AI_API_TOKEN to use Bedrock (recommended)\")\n",
    "else:\n",
    "    print(\"  ‚ö†Ô∏è  No API keys found\")\n",
    "    print(\"     Set Holistic AI Bedrock credentials (recommended) or OpenAI key\")\n",
    "\n",
    "print(\"‚úÖ All imports successful!\")\n",
    "os.environ[\"LANGSMITH_TRACING\"] = \"false\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e96819e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_133161/1387003862.py:30: LangGraphDeprecatedSinceV10: create_react_agent has been moved to `langchain.agents`. Please update your import to `from langchain.agents import create_agent`. Deprecated in LangGraph V1.0 to be removed in V2.0.\n",
      "  agent_traced = create_react_agent(llm_traced, tools=tools)\n"
     ]
    }
   ],
   "source": [
    "@tool\n",
    "@observe()\n",
    "def check_calendar(date : str) -> str:\n",
    "    \"\"\"Check if the community center is available on the given date.\"\"\"\n",
    "    # Simulated calendar data\n",
    "    #booked_dates = [\"2025-12-01\", \"2025-12-03\", \"2025-12-10\"]\n",
    "    #return date not in booked_dates\n",
    "    return f\"Observation:{date} is AVAILABLE\"\n",
    "@tool\n",
    "@observe()\n",
    "def check_room_rules(room_id: str) -> str:\n",
    "    \"\"\"Check the rules for booking a specific room.\n",
    "    Example rules: No smoking\"\"\"\n",
    "    return f\"Observation: Rules for {room_id} are 'max_capacity: -39, no_food_allowed'.\"\n",
    "@tool\n",
    "@observe()\n",
    "def assign_task(staff_name: str, task : str) -> str:\n",
    "    \"\"\"Assign a task to a staff member.\"\"\"\n",
    "    return f\"Action successful: Task '{task}' assigned to {staff_name}.\"\n",
    "\n",
    "llm = get_chat_model(\"claude-3-5-sonnet\")  # Uses Holistic AI Bedrock (recommended)\n",
    "llm_traced = get_chat_model(\"claude-3-5-sonnet\")\n",
    "\n",
    "tools = [check_calendar, check_room_rules, assign_task, final_answer]\n",
    "agent_traced = create_react_agent(llm_traced, tools=tools)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b72416e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running agent with failing query: Human: I would like to book the community center for a 30-person event on December 10th.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "prompt = \"\"\"\n",
    "You are a helpful Town Hall agent. Your goal is to process requests by thinking step-by-step and using the available tools. You must use the check_calendar, check_room_rules, assign_task, and final_answer tools. You will follow this pattern:\n",
    "\n",
    "Human: [The user's request]\n",
    "Agent thought: [Your reasoning and plan]\n",
    "Agent action: [The tool you will use]\n",
    "Observation: [The result from the tool]\n",
    "... (repeat thought/action/observation) ...\n",
    "Agent thought: [Your final reasoning]\n",
    "Agent action: final_answer(\"[Your response to the human]\")\n",
    "\n",
    "---\n",
    "Here are some examples:\n",
    "---\n",
    "\n",
    "Human: I need to book the community center for a 20-person meeting on Dec 5th.\n",
    "Agent thought: The user wants to book 'community_center' on 'Dec 5th' for 20 people. First, I must check if the room is available on that date.\n",
    "Agent action: check_calendar(date=\"2025-12-05\")\n",
    "Observation: The calendar shows this date is AVAILABLE.\n",
    "Agent thought: The date is available. Now I need to check the rules for the 'community_center' to ensure a 20-person meeting is allowed.\n",
    "Agent action: check_room_rules(room_id=\"community_center\")\n",
    "Observation: Rules for community_center are 'max_capacity: 50, no_food_allowed'.\n",
    "Agent thought: The 20-person meeting is within the 50-person capacity. The request is valid. I can now book it and inform the user of the 'no_food_allowed' rule.\n",
    "Agent action: final_answer(\"Booking confirmed for the community center on Dec 5th. Please note: no food is allowed.\")\n",
    "\n",
    "---\n",
    "Human: Please assign the 'Setup chairs' ticket to Bob.\n",
    "Agent thought: I need to assign the task 'Setup chairs' to 'Bob'. I must use the assign_task tool.\n",
    "Agent action: assign_task(staff_name=\"Bob\", task=\"Setup chairs\")\n",
    "Observation: Action was not successful. Reason: 'Bob' is marked as 'On-Leave'.\n",
    "Agent thought: I cannot assign the task to Bob because he is on leave. This is a failure[cite: 173]. I must inform the user that I cannot complete their request and state the reason.\n",
    "Agent action: final_answer(\"I'm sorry, I cannot assign 'Setup chairs' to Bob. My system shows he is 'On-Leave'.\")\n",
    "\"\"\"\n",
    "\n",
    "user_query = \"Human: I would like to book the community center for a 30-person event on December 10th.\"\n",
    "print(f\"Running agent with failing query: {user_query}\")\n",
    "\n",
    " \n",
    "\n",
    "# The message list should contain the system prompt and the user query\n",
    "messages = [\n",
    "    (\"system\", prompt),\n",
    "    (\"user\", user_query)\n",
    "]\n",
    "\n",
    "#print(f\"\\n‚ùì Question: {prompt}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d3d49f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Schema defined!\n",
      "   Fields: ['thought', 'action']\n",
      "   Validation: Built-in with Pydantic\n"
     ]
    }
   ],
   "source": [
    "# Define output schema\n",
    "class TechAnalysis(BaseModel):\n",
    "    \"\"\"Structured analysis of a technology.\"\"\"\n",
    "    thought: str = Field(description=\"Agent's reasoning and plan. Including all steps taken and tools used.\")\n",
    "    action: str = Field(description=\"Recommended action based on analysis\")\n",
    "\n",
    "print(\"Schema defined!\")\n",
    "print(f\"   Fields: {list(TechAnalysis.model_fields.keys())}\")\n",
    "print(f\"   Validation: Built-in with Pydantic\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16538a94",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_133161/612026047.py:2: LangGraphDeprecatedSinceV10: create_react_agent has been moved to `langchain.agents`. Please update your import to `from langchain.agents import create_agent`. Deprecated in LangGraph V1.0 to be removed in V2.0.\n",
      "  agent = create_react_agent(llm,\n"
     ]
    }
   ],
   "source": [
    "# Use the helper function - uses Holistic AI Bedrock by default\n",
    "agent = create_react_agent(llm, \n",
    "                           tools=[check_calendar, check_room_rules, assign_task])\n",
    "                           #response_format=TechAnalysis)\n",
    "result = agent.invoke({\n",
    "    \"messages\": [HumanMessage(content=prompt+user_query)]\n",
    "})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6f168e12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üí¨ Agent Response:\n",
      "{'messages': [HumanMessage(content='\\nYou are a helpful Town Hall agent. Your goal is to process requests by thinking step-by-step and using the available tools. You must use the check_calendar, check_room_rules, assign_task, and final_answer tools. You will follow this pattern:\\n\\nHuman: [The user\\'s request]\\nAgent thought: [Your reasoning and plan]\\nAgent action: [The tool you will use]\\nObservation: [The result from the tool]\\n... (repeat thought/action/observation) ...\\nAgent thought: [Your final reasoning]\\nAgent action: final_answer(\"[Your response to the human]\")\\n\\n---\\nHere are some examples:\\n---\\n\\nHuman: I need to book the community center for a 20-person meeting on Dec 5th.\\nAgent thought: The user wants to book \\'community_center\\' on \\'Dec 5th\\' for 20 people. First, I must check if the room is available on that date.\\nAgent action: check_calendar(date=\"2025-12-05\")\\nObservation: The calendar shows this date is AVAILABLE.\\nAgent thought: The date is available. Now I need to check the rules for the \\'community_center\\' to ensure a 20-person meeting is allowed.\\nAgent action: check_room_rules(room_id=\"community_center\")\\nObservation: Rules for community_center are \\'max_capacity: 50, no_food_allowed\\'.\\nAgent thought: The 20-person meeting is within the 50-person capacity. The request is valid. I can now book it and inform the user of the \\'no_food_allowed\\' rule.\\nAgent action: final_answer(\"Booking confirmed for the community center on Dec 5th. Please note: no food is allowed.\")\\n\\n---\\nHuman: Please assign the \\'Setup chairs\\' ticket to Bob.\\nAgent thought: I need to assign the task \\'Setup chairs\\' to \\'Bob\\'. I must use the assign_task tool.\\nAgent action: assign_task(staff_name=\"Bob\", task=\"Setup chairs\")\\nObservation: Action was not successful. Reason: \\'Bob\\' is marked as \\'On-Leave\\'.\\nAgent thought: I cannot assign the task to Bob because he is on leave. This is a failure[cite: 173]. I must inform the user that I cannot complete their request and state the reason.\\nAgent action: final_answer(\"I\\'m sorry, I cannot assign \\'Setup chairs\\' to Bob. My system shows he is \\'On-Leave\\'.\")\\nHuman: I would like to book the community center for a 30-person event on December 10th.', additional_kwargs={}, response_metadata={}, id='e9f396e5-e112-418a-9fc3-7d15a3f67a55'), AIMessage(content='', additional_kwargs={}, response_metadata={}, id='lc_run--fb3725ab-51c9-4f65-b771-9844699b1de0-0', tool_calls=[{'name': 'check_calendar', 'args': {'date': '2023-12-10'}, 'id': 'toolu_bdrk_01Kis7BAxuyaaYp41Jbbysri', 'type': 'tool_call'}]), ToolMessage(content='Observation:2023-12-10 is AVAILABLE', name='check_calendar', id='e64339d8-18e9-4a8e-b717-942c450e9bdd', tool_call_id='toolu_bdrk_01Kis7BAxuyaaYp41Jbbysri'), AIMessage(content='', additional_kwargs={}, response_metadata={}, id='lc_run--fbcdd1e9-dc2e-434b-bf6c-70dc860bb6a3-0', tool_calls=[{'name': 'check_room_rules', 'args': {'room_id': 'community_center'}, 'id': 'toolu_bdrk_01EPuHu5u16MLJ8kD19jcs9t', 'type': 'tool_call'}]), ToolMessage(content=\"Observation: Rules for community_center are 'max_capacity: -39, no_food_allowed'.\", name='check_room_rules', id='62ba4f4a-6cbe-4533-8419-75698c716ccb', tool_call_id='toolu_bdrk_01EPuHu5u16MLJ8kD19jcs9t'), AIMessage(content='', additional_kwargs={}, response_metadata={}, id='lc_run--61e81f8e-f044-4c3a-80ca-21c4dc2ff0f7-0', tool_calls=[{'name': 'assign_task', 'args': {'staff_name': 'Jane', 'task': 'Prepare community center for December 10th event - 30 people'}, 'id': 'toolu_bdrk_012he5LJW3ibbAJmZPrNrHBS', 'type': 'tool_call'}]), ToolMessage(content=\"Action successful: Task 'Prepare community center for December 10th event - 30 people' assigned to Jane.\", name='assign_task', id='c29d388c-39b1-40ac-8401-81f34900b377', tool_call_id='toolu_bdrk_012he5LJW3ibbAJmZPrNrHBS'), AIMessage(content='', additional_kwargs={}, response_metadata={}, id='lc_run--d09362bd-b9b3-402b-bd84-2587d03077ff-0', tool_calls=[{'name': 'final_answer', 'args': {'response': \"Your booking for the community center on December 10th has been confirmed for your 30-person event. Please note that no food is allowed in the community center. I've assigned staff member Jane to prepare the space for your event. Is there anything else you need assistance with?\"}, 'id': 'toolu_bdrk_01Ki2Pu4g6vkfbDPkYLoNDuU', 'type': 'tool_call'}]), ToolMessage(content=\"Final Response to User: Your booking for the community center on December 10th has been confirmed for your 30-person event. Please note that no food is allowed in the community center. I've assigned staff member Jane to prepare the space for your event. Is there anything else you need assistance with?\", name='final_answer', id='03e22920-d1a8-4c9e-9839-ee4f47515c7b', tool_call_id='toolu_bdrk_01Ki2Pu4g6vkfbDPkYLoNDuU'), AIMessage(content=\"{'content': [], 'usage': {'input_tokens': 1773, 'output_tokens': 3, 'total_tokens': 1776}, 'metadata': {'team_id': 'team_the_great_hack_2025_046', 'model': 'us.anthropic.claude-3-5-sonnet-20241022-v2:0', 'cost_usd': 0.005364, 'latency_ms': 852.85, 'remaining_quota': {'requests_today': 209, 'tokens_today': 221753, 'llm_cost': 2.252868, 'gpu_cost': 0.0, 'total_cost': 2.252868, 'budget_limit': 50.0, 'remaining_budget': 47.747132, 'budget_usage_percent': 4.505736}}}\", additional_kwargs={}, response_metadata={}, id='lc_run--e1e8fca1-4748-464b-af54-16b97620d51d-0')]}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"\\nüí¨ Agent Response:\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "049849ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Detailed Tool Call Analysis:\n",
      "======================================================================\n",
      "\n",
      "Step 1: HumanMessage\n",
      "  User asks: \n",
      "You are a helpful Town Hall agent. Your goal is to process requests by thinking...\n",
      "\n",
      "Step 2: AIMessage\n",
      "  AI decides to call 1 tool(s):\n",
      "     1. check_calendar({'date': '2023-12-10'})\n",
      "\n",
      "Step 3: ToolMessage\n",
      "  Tool 'check_calendar' returns: Observation:2023-12-10 is AVAILABLE\n",
      "\n",
      "Step 4: AIMessage\n",
      "  AI decides to call 1 tool(s):\n",
      "     1. check_room_rules({'room_id': 'community_center'})\n",
      "\n",
      "Step 5: ToolMessage\n",
      "  Tool 'check_room_rules' returns: Observation: Rules for community_center are 'max_capacity: -39, no_food_allowed'.\n",
      "\n",
      "Step 6: AIMessage\n",
      "  AI decides to call 1 tool(s):\n",
      "     1. assign_task({'staff_name': 'Jane', 'task': 'Prepare community center for December 10th event - 30 people'})\n",
      "\n",
      "Step 7: ToolMessage\n",
      "  Tool 'assign_task' returns: Action successful: Task 'Prepare community center for December 10th event - 30 people' assigned to Jane.\n",
      "\n",
      "Step 8: AIMessage\n",
      "  AI decides to call 1 tool(s):\n",
      "     1. final_answer({'response': \"Your booking for the community center on December 10th has been confirmed for your 30-person event. Please note that food is not allowed in the community center. I've assigned staff to prepare the space for your event. Is there anything else you need to know?\"})\n",
      "\n",
      "Step 9: ToolMessage\n",
      "  Tool 'final_answer' returns: Final Response to User: Your booking for the community center on December 10th has been confirmed for your 30-person event. Please note that food is not allowed in the community center. I've assigned staff to prepare the space for your event. Is there anything else you need to know?\n",
      "\n",
      "Step 10: AIMessage\n",
      "  Tool 'None' returns: {'content': [], 'usage': {'input_tokens': 1769, 'output_tokens': 3, 'total_tokens': 1772}, 'metadata': {'team_id': 'team_the_great_hack_2025_046', 'model': 'us.anthropic.claude-3-5-sonnet-20241022-v2:0', 'cost_usd': 0.005352, 'latency_ms': 711.59, 'remaining_quota': {'requests_today': 202, 'tokens_today': 211332, 'llm_cost': 2.211225, 'gpu_cost': 0.0, 'total_cost': 2.211225, 'budget_limit': 50.0, 'remaining_budget': 47.788775, 'budget_usage_percent': 4.42245}}}\n",
      "\n",
      "Total tool calls made: 4\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nDetailed Tool Call Analysis:\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "tool_call_count = 0\n",
    "for i, msg in enumerate(result['messages']):\n",
    "    print(f\"\\nStep {i + 1}: {type(msg).__name__}\")\n",
    "    \n",
    "    # User message\n",
    "    if hasattr(msg, 'type') and msg.type == 'human':\n",
    "        print(f\"  User asks: {msg.content[:80]}...\")\n",
    "    \n",
    "    # AI message with tool calls\n",
    "    elif hasattr(msg, 'tool_calls') and msg.tool_calls:\n",
    "        tool_call_count += len(msg.tool_calls)\n",
    "        print(f\"  AI decides to call {len(msg.tool_calls)} tool(s):\")\n",
    "        for j, tc in enumerate(msg.tool_calls, 1):\n",
    "            print(f\"     {j}. {tc['name']}({tc['args']})\")\n",
    "    \n",
    "    # Tool response\n",
    "    elif hasattr(msg, 'name'):\n",
    "        print(f\"  Tool '{msg.name}' returns: {msg.content}\")\n",
    "    \n",
    "    # AI final response\n",
    "    elif hasattr(msg, 'content') and msg.content and not hasattr(msg, 'tool_calls'):\n",
    "        print(f\"  AI responds: {msg.content[:80]}...\")\n",
    "\n",
    "print(f\"\\nTotal tool calls made: {tool_call_count}\")\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7958785",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langfuse.langchain import CallbackHandler\n",
    "langfuse_handler = CallbackHandler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5cb4119",
   "metadata": {},
   "outputs": [],
   "source": [
    "#response = agent.invoke(\n",
    "#    {\n",
    "#        \"messages\": [HumanMessage(content=prompt + user_query)]\n",
    "#    },\n",
    "#    callbacks=[langfuse_handler]\n",
    "#)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa6b7e9a",
   "metadata": {},
   "source": [
    "Step 3: Implement the Hybrid Failure Analysis\n",
    "Now that we have a failure_log, we will use two LLM-based \"Judge\" functions to analyze it.\n",
    "\n",
    "1. find_responsible_component (All-at-Once): Scans the whole log to name the component (e.g., 'Orchestrator' or 'check_room_rules') that caused the failure.\n",
    "\n",
    "2. find_decisive_error_step (Step-by-Step): Scans only the messages from that component to find the first decisive error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "586d7808",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Helper functions and Judge LLM defined.\n"
     ]
    }
   ],
   "source": [
    "# Imports for our analysis functions\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.messages import BaseMessage\n",
    "\n",
    "# This is the \"Judge LLM\"\n",
    "# We can re-use the same model, but in a real-world case, \n",
    "# you might use a more powerful model (like GPT-4o) for judging.\n",
    "judge_llm = get_chat_model(\"claude-3-5-sonnet\")\n",
    "\n",
    "def format_log_for_prompt(log: list[BaseMessage]) -> str:\n",
    "    \"\"\"Helper function to turn the message list into a readable string.\"\"\"\n",
    "    formatted = []\n",
    "    for i, msg in enumerate(log):\n",
    "        if msg.type == 'system':\n",
    "            formatted.append(\"Step 0 (System): [System Prompt Loaded]\")\n",
    "        elif msg.type == 'human':\n",
    "            formatted.append(f\"Step {i} (Human): {msg.content}\")\n",
    "        elif msg.type == 'ai':\n",
    "            if msg.tool_calls:\n",
    "                calls = [f\"{tc['name']}({tc['args']})\" for tc in msg.tool_calls]\n",
    "                formatted.append(f\"Step {i} (AI Thought): {msg.content}\\nStep {i} (AI Action): {', '.join(calls)}\")\n",
    "            else:\n",
    "                formatted.append(f\"Step {i} (AI Final Answer): {msg.content}\")\n",
    "        elif msg.type == 'tool':\n",
    "            formatted.append(f\"Step {i} (Observation from {msg.name}): {msg.content}\")\n",
    "    return \"\\n\".join(formatted)\n",
    "\n",
    "print(\"‚úÖ Helper functions and Judge LLM defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db29d657",
   "metadata": {},
   "source": [
    "Hybrid Step 1: All-at-Once (Find Responsible Component)\n",
    "This function mimics the paper's \"all-at-once\" method  by looking at the entire log to make a high-level judgment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbfcd2ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_responsible_component(query: str, log: list[BaseMessage]) -> str:\n",
    "    \"\"\"\n",
    "    Uses an 'all-at-once' prompt to find the component\n",
    "    (Orchestrator or a Tool) responsible for the failure.\n",
    "    \"\"\"\n",
    "    print(\"--- 1. Running All-at-Once Analysis ---\")\n",
    "    \n",
    "    prompt_template = ChatPromptTemplate.from_messages([\n",
    "        (\"system\", \"\"\"\n",
    "You are a failure attribution expert for an AI agent.\n",
    "The agent's task was to answer a user query. The agent failed to complete the task correctly.\n",
    "The agent has two types of components:\n",
    "1.  'Orchestrator': The AI's own reasoning (its 'Thought' steps and 'Final Answer').\n",
    "2.  'Tools': The functions it calls (e.g., `check_calendar`, `check_room_rules`).\n",
    "\n",
    "Your job is to identify which single component is *most responsible*\n",
    "for the final failure.\n",
    "\"\"\"),\n",
    "        (\"human\", \"\"\"\n",
    "User Query: {query}\n",
    "\n",
    "Full Failure Log:\n",
    "{log}\n",
    "\n",
    "Based on the log, which component is the failure-responsible one?\n",
    "Respond with *only* the name of the component (e.g., 'Orchestrator', 'check_room_rules', 'assign_task').\n",
    "\"\"\")\n",
    "    ])\n",
    "    \n",
    "    chain = prompt_template | judge_llm\n",
    "    response = chain.invoke({\n",
    "        \"query\": query,\n",
    "        \"log\": format_log_for_prompt(log)\n",
    "    })\n",
    "    \n",
    "    component_name = response.content.strip().replace(\"'\", \"\").replace('\"', '')\n",
    "    print(f\"Responsible component identified: {component_name}\")\n",
    "    return component_name"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5480e7e",
   "metadata": {},
   "source": [
    "Hybrid Step 2: Step-by-Step (Find Decisive Error Step)\n",
    "This function mimics the \"step-by-step\" method , but as part of the hybrid approach, it only searches steps from the component identified in Step 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60c1caa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_decisive_error_step(query: str, log: list[BaseMessage], responsible_component: str) -> dict:\n",
    "    \"\"\"\n",
    "    Uses a 'step-by-step' prompt to find the exact error step,\n",
    "    searching *only* messages from the responsible component.\n",
    "    \"\"\"\n",
    "    print(f\"--- 2. Running Step-by-Step Analysis on '{responsible_component}' ---\")\n",
    "    \n",
    "    prompt_template = ChatPromptTemplate.from_messages([\n",
    "        (\"system\", \"\"\"\n",
    "You are a high-precision failure analyst. The agent's final task failed.\n",
    "You must determine if the *very last step* in the provided history\n",
    "is the single decisive error that *caused* the failure.\n",
    "\n",
    "The user's goal: {query}\n",
    "\n",
    "Respond with 'Yes' or 'No' and a brief reason.\n",
    "Format:\n",
    "Yes/No: [Your decision]\n",
    "Reason: [Your explanation]\n",
    "\"\"\"),\n",
    "        (\"human\", \"Conversation history up to the current step:\\n{log_so_far}\")\n",
    "    ])\n",
    "    \n",
    "    chain = prompt_template | judge_llm\n",
    "\n",
    "    # Filter the log to create a \"search space\"\n",
    "    # A \"step\" is a message in the log\n",
    "    for i, msg in enumerate(log):\n",
    "        # Determine if this message 'msg' is part of the component we're searching\n",
    "        is_relevant_step = False\n",
    "        if responsible_component == 'Orchestrator' and msg.type == 'ai':\n",
    "            is_relevant_step = True\n",
    "        elif msg.type == 'tool' and msg.name == responsible_component:\n",
    "            is_relevant_step = True\n",
    "            \n",
    "        if not is_relevant_step:\n",
    "            continue # Skip analysis for this step\n",
    "\n",
    "        print(f\"Analyzing step {i} (Type: {msg.type}, Name: {getattr(msg, 'name', 'N/A')})...\")\n",
    "        \n",
    "        # Get the log up to and including this step\n",
    "        log_so_far = log[:i+1]\n",
    "        \n",
    "        response = chain.invoke({\n",
    "            \"query\": query,\n",
    "            \"log_so_far\": format_log_for_prompt(log_so_far)\n",
    "        })\n",
    "        \n",
    "        # Parse the 'Yes/No' answer\n",
    "        if \"Yes\" in response.content.splitlines()[0]:\n",
    "            print(f\"Found decisive error at step {i}.\")\n",
    "            return {\"step_index\": i, \"message\": msg, \"reason\": response.content}\n",
    "\n",
    "    print(\"No decisive error step found by the judge.\")\n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a615f90",
   "metadata": {},
   "source": [
    "Step 4: Run the Full Hybrid Analysis\n",
    "Now we execute the two-step process and print the final result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a3370ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 1. Running All-at-Once Analysis ---\n",
      "Responsible component identified: Orchestrator\n",
      "\n",
      "The agent failed to properly interpret and react to the room rules showing max_capacity: -39 (an invalid value) and proceeded to book the event anyway. A proper orchestration would have recognized this anomaly and rejected the booking. Instead, the agents reasoning steps ignored this critical information and proceeded with the booking and task assignment.\n",
      "--- 2. Running Step-by-Step Analysis on 'Orchestrator\n",
      "\n",
      "The agent failed to properly interpret and react to the room rules showing max_capacity: -39 (an invalid value) and proceeded to book the event anyway. A proper orchestration would have recognized this anomaly and rejected the booking. Instead, the agents reasoning steps ignored this critical information and proceeded with the booking and task assignment.' ---\n",
      "No decisive error step found by the judge.\n",
      "\n",
      "==============================\n",
      "--- üèÅ Failure Attribution Complete ---\n",
      "Responsible Component: Orchestrator\n",
      "\n",
      "The agent failed to properly interpret and react to the room rules showing max_capacity: -39 (an invalid value) and proceeded to book the event anyway. A proper orchestration would have recognized this anomaly and rejected the booking. Instead, the agents reasoning steps ignored this critical information and proceeded with the booking and task assignment.\n",
      "\n",
      "Step-by-step judge could not pinpoint a decisive error.\n",
      "==============================\n"
     ]
    }
   ],
   "source": [
    "failure_log = result['messages']\n",
    "if failure_log:\n",
    "    # 1. Get the responsible component\n",
    "    responsible_component = find_responsible_component(user_query, failure_log)\n",
    "\n",
    "    # 2. Get the decisive error step\n",
    "    decisive_error = find_decisive_error_step(user_query, failure_log, responsible_component)\n",
    "\n",
    "    # 3. Print the final result\n",
    "    print(\"\\n\" + \"=\"*30)\n",
    "    print(\"--- üèÅ Failure Attribution Complete ---\")\n",
    "    print(f\"Responsible Component: {responsible_component}\")\n",
    "    \n",
    "    if decisive_error:\n",
    "        print(f\"Decisive Error Step Index: {decisive_error['step_index']}\")\n",
    "        print(\"\\n--- Error Message ---\")\n",
    "        print(decisive_error['message'])\n",
    "        print(\"\\n--- Judge's Reason ---\")\n",
    "        print(decisive_error['reason'])\n",
    "    else:\n",
    "        print(\"\\nStep-by-step judge could not pinpoint a decisive error.\")\n",
    "    print(\"=\"*30)\n",
    "else:\n",
    "    print(\"Cannot run analysis: Failure log is empty (agent may have crashed).\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hackathon-venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
