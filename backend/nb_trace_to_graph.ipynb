{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae1a8889",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "from typing import Optional\n",
    "from trace_to_graph import TraceGraphBuilder\n",
    "import json\n",
    "from typing import List, Dict\n",
    "\n",
    "\n",
    "class TraceGraphBuilder:\n",
    "    def __init__(self, include_messages: bool = False):\n",
    "        self.include_messages = include_messages\n",
    "        self.nodes = []\n",
    "        self.edges = []\n",
    "        self.runs = []\n",
    "    \n",
    "    def load_trace(self, trace_path: str | Path) -> List[Dict]:\n",
    "        \"\"\"Load a trace JSON file.\"\"\"\n",
    "        with open(trace_path, 'r') as f:\n",
    "            return json.load(f)\n",
    "\n",
    "    def build_graph(self, trace_data: List[Dict]) -> None:\n",
    "        \"\"\"Build a graph from the trace data.\"\"\"\n",
    "        # for run in trace_data:\n",
    "        #     self.runs.append(run)\n",
    "        #     self.nodes.append(run)\n",
    "        #     self.edges.append(run)\n",
    "    \n",
    "\n",
    "from collections import defaultdict\n",
    "def build_graph(runs):\n",
    "    graph = defaultdict(list)\n",
    "    id_to_run = {r[\"id\"]: r for r in runs}\n",
    "    for r in runs:\n",
    "        for parent in r.get(\"parent_run_ids\", []):\n",
    "            graph[parent].append(r[\"id\"])\n",
    "    return graph, id_to_run\n",
    "\n",
    "def find_roots(runs):\n",
    "    return [r[\"id\"] for r in runs if not r.get(\"parent_run_ids\")]\n",
    "\n",
    "def traverse(graph, id_to_run, node_id, depth=0, chain=None):\n",
    "    if chain is None:\n",
    "        chain = []\n",
    "    run = id_to_run[node_id]\n",
    "    step = {\n",
    "        \"depth\": depth,\n",
    "        \"name\": run.get(\"name\"),\n",
    "        \"run_type\": run.get(\"run_type\"),\n",
    "        \"input\": run.get(\"inputs\"),\n",
    "        \"output\": run.get(\"outputs\"),\n",
    "        \"start_time\": run.get(\"start_time\")\n",
    "    }\n",
    "    chain.append(step)\n",
    "    for child in graph.get(node_id, []):\n",
    "        traverse(graph, id_to_run, child, depth+1, chain)\n",
    "    return chain\n",
    "\n",
    "def chain_runs(runs):\n",
    "    graph, id_to_run = build_graph(runs)\n",
    "    roots = find_roots(runs)\n",
    "    full_chain = []\n",
    "    for root in roots:\n",
    "        chain = traverse(graph, id_to_run, root)\n",
    "        full_chain.extend(chain)\n",
    "    return sorted(full_chain, key=lambda x: x[\"start_time\"])\n",
    "\n",
    "def dedupe_trace(chain):\n",
    "    cleaned = []\n",
    "    last_payload = None\n",
    "\n",
    "    for step in chain:\n",
    "        name = step[\"name\"]\n",
    "        inp = step.get(\"input\")\n",
    "        out = step.get(\"output\")\n",
    "\n",
    "        # drop nodes with no useful I/O\n",
    "        if not inp and not out:\n",
    "            continue\n",
    "\n",
    "        # 4. collapse exact duplicate steps\n",
    "        current_payload = (name, str(inp), str(out))\n",
    "        if current_payload == last_payload:\n",
    "            continue\n",
    "        last_payload = current_payload\n",
    "\n",
    "        cleaned.append(step)\n",
    "\n",
    "    return cleaned\n",
    "\n",
    "def summarize_chain(chain):\n",
    "    chain = dedupe_trace(chain)\n",
    "    for i,step in enumerate(chain):\n",
    "        indent = \"  \" * step[\"depth\"]\n",
    "        if i == 0:\n",
    "            if step.get(\"input\"):\n",
    "                print(f\"> input: {step['input']}\")\n",
    "\n",
    "        print(f\"{indent}- {step['name']} ({step['run_type']})\")\n",
    "        if step.get(\"input\"):\n",
    "            msg = str(step[\"input\"])[:120]\n",
    "            print(f\"{indent}  ↳ input: {msg}\")\n",
    "        if step.get(\"output\"):\n",
    "            msg = str(step[\"output\"])[:120]\n",
    "            print(f\"{indent}  ↳ output: {msg}\")\n",
    "\n",
    "        \n",
    "        if i == len(chain) - 1:\n",
    "            if step.get(\"output\"):\n",
    "                print(f\"> output: {step['output']}\")\n",
    "\n",
    "\n",
    "\n",
    "def visualize_trace(trace_path: str | Path, output_path: Optional[str] = None, \n",
    "                   include_messages: bool = False, interactive: bool = True) -> None:\n",
    "    \"\"\"\n",
    "    Load a trace file and visualize it as a graph.\n",
    "    \n",
    "    Args:\n",
    "        trace_path: Path to the trace JSON file\n",
    "        output_path: Path to save the visualization (optional)\n",
    "        include_messages: Whether to include individual messages as nodes\n",
    "        interactive: If True, use pyvis for interactive HTML. If False, use matplotlib.\n",
    "    \"\"\"\n",
    "    builder = TraceGraphBuilder(include_messages=include_messages)\n",
    "    trace_data = builder.load_trace(trace_path)\n",
    "    builder.build_graph(trace_data)\n",
    "    summarize_chain(chain_runs(trace_data))\n",
    "    # for t in trace_data:\n",
    "    #     step = extract_step(t)\n",
    "    #     for k, v in step.items():\n",
    "    #         if type(v) == list:\n",
    "    #             print(k)\n",
    "    #             for i in v:\n",
    "    #                 print(i)\n",
    "    #         else:\n",
    "    #             print(k, v)\n",
    "    #     print(\"Next step --------------------------------\")\n",
    "    # print (\"--------------------------------\")\n",
    "    \n",
    "    print(f\"Loaded trace with {len(builder.runs)} runs\")\n",
    "    print(f\"Created {len(builder.nodes)} nodes and {len(builder.edges)} edges\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee79bab7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e8db06b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from typing import List, Dict, Any, Optional, Tuple\n",
    "from langchain_core.messages import HumanMessage, AIMessage, ToolMessage, BaseMessage\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "# Import the model helper (adjust import path as needed)\n",
    "try:\n",
    "    from holistic_ai_bedrock import get_chat_model\n",
    "    HAS_BEDROCK = True\n",
    "except ImportError:\n",
    "    try:\n",
    "        from langchain_openai import ChatOpenAI\n",
    "        HAS_BEDROCK = False\n",
    "    except ImportError:\n",
    "        print(\"Warning: No LLM provider found. Install holistic_ai_bedrock or langchain_openai\")\n",
    "        HAS_BEDROCK = None\n",
    "\n",
    "\n",
    "def extract_messages_from_trace(trace_data: List[Dict]) -> List[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Extract all messages from trace data, including from inputs and outputs.\n",
    "    Returns a list of message dictionaries with metadata.\n",
    "    \"\"\"\n",
    "    messages = []\n",
    "    \n",
    "    for run in trace_data:\n",
    "        run_id = run.get(\"id\")\n",
    "        run_name = run.get(\"name\", \"Unknown\")\n",
    "        run_type = run.get(\"run_type\", \"unknown\")\n",
    "        start_time = run.get(\"start_time\")\n",
    "        \n",
    "        # Extract messages from inputs\n",
    "        inputs = run.get(\"inputs\", {})\n",
    "        if isinstance(inputs, dict):\n",
    "            # Handle different input structures\n",
    "            if \"messages\" in inputs:\n",
    "                msgs = inputs[\"messages\"]\n",
    "                # Handle nested list structure\n",
    "                if isinstance(msgs, list) and len(msgs) > 0:\n",
    "                    if isinstance(msgs[0], list):\n",
    "                        msgs = msgs[0]  # Unwrap nested list\n",
    "                    for msg in msgs:\n",
    "                        if isinstance(msg, dict):\n",
    "                            messages.append({\n",
    "                                \"message\": msg,\n",
    "                                \"source\": \"input\",\n",
    "                                \"run_id\": run_id,\n",
    "                                \"run_name\": run_name,\n",
    "                                \"run_type\": run_type,\n",
    "                                \"timestamp\": start_time\n",
    "                            })\n",
    "            elif \"input\" in inputs and isinstance(inputs[\"input\"], dict):\n",
    "                # Handle nested input structure\n",
    "                nested_input = inputs[\"input\"]\n",
    "                if \"messages\" in nested_input:\n",
    "                    msgs = nested_input[\"messages\"]\n",
    "                    if isinstance(msgs, list) and len(msgs) > 0:\n",
    "                        if isinstance(msgs[0], list):\n",
    "                            msgs = msgs[0]\n",
    "                        for msg in msgs:\n",
    "                            if isinstance(msg, dict):\n",
    "                                messages.append({\n",
    "                                    \"message\": msg,\n",
    "                                    \"source\": \"input\",\n",
    "                                    \"run_id\": run_id,\n",
    "                                    \"run_name\": run_name,\n",
    "                                    \"run_type\": run_type,\n",
    "                                    \"timestamp\": start_time\n",
    "                                })\n",
    "        \n",
    "        # Extract messages from outputs\n",
    "        outputs = run.get(\"outputs\", {})\n",
    "        if isinstance(outputs, dict):\n",
    "            if \"messages\" in outputs:\n",
    "                msgs = outputs[\"messages\"]\n",
    "                if isinstance(msgs, list) and len(msgs) > 0:\n",
    "                    if isinstance(msgs[0], list):\n",
    "                        msgs = msgs[0]\n",
    "                    for msg in msgs:\n",
    "                        if isinstance(msg, dict):\n",
    "                            messages.append({\n",
    "                                \"message\": msg,\n",
    "                                \"source\": \"output\",\n",
    "                                \"run_id\": run_id,\n",
    "                                \"run_name\": run_name,\n",
    "                                \"run_type\": run_type,\n",
    "                                \"timestamp\": start_time\n",
    "                            })\n",
    "    \n",
    "    return messages\n",
    "\n",
    "\n",
    "def format_message_for_analysis(msg_dict: Dict[str, Any]) -> str:\n",
    "    \"\"\"Format a message dictionary into a readable string for LLM analysis.\"\"\"\n",
    "    msg = msg_dict[\"message\"]\n",
    "    msg_type = msg.get(\"type\", \"unknown\")\n",
    "    content = msg.get(\"content\", \"\")\n",
    "    \n",
    "    # Handle tool calls\n",
    "    tool_calls = msg.get(\"tool_calls\", [])\n",
    "    if tool_calls:\n",
    "        tool_info = []\n",
    "        for tc in tool_calls:\n",
    "            tool_name = tc.get(\"name\", \"unknown\")\n",
    "            tool_args = tc.get(\"args\", {})\n",
    "            tool_info.append(f\"{tool_name}({tool_args})\")\n",
    "        content += f\" [Tool calls: {', '.join(tool_info)}]\"\n",
    "    \n",
    "    # Handle additional kwargs\n",
    "    additional_kwargs = msg.get(\"additional_kwargs\", {})\n",
    "    if additional_kwargs.get(\"tool_calls\"):\n",
    "        tool_info = []\n",
    "        for tc in additional_kwargs[\"tool_calls\"]:\n",
    "            func = tc.get(\"function\", {})\n",
    "            tool_name = func.get(\"name\", \"unknown\")\n",
    "            tool_args = func.get(\"arguments\", \"{}\")\n",
    "            tool_info.append(f\"{tool_name}({tool_args})\")\n",
    "        content += f\" [Tool calls: {', '.join(tool_info)}]\"\n",
    "    \n",
    "    # Build formatted string\n",
    "    formatted = f\"[{msg_dict['source'].upper()}] {msg_type.upper()}\"\n",
    "    if msg_dict.get(\"run_name\"):\n",
    "        formatted += f\" from {msg_dict['run_name']}\"\n",
    "    formatted += f\": {content}\"\n",
    "    \n",
    "    return formatted\n",
    "\n",
    "\n",
    "def find_issue_origin(\n",
    "    trace_data: List[Dict] | Dict[str, Any],\n",
    "    user_question: str,\n",
    "    llm_model: Optional[Any] = None,\n",
    "    max_messages_to_check: int = 20,\n",
    "    confidence_threshold: float = 0.7\n",
    ") -> List[Tuple[Dict[str, Any], float, str]]:\n",
    "    \"\"\"\n",
    "    Analyze trace messages in reverse order to find the origin of an issue.\n",
    "    \n",
    "    Args:\n",
    "        trace_data: Either a list of trace run dictionaries, or a dict with 'messages' key\n",
    "                   containing LangChain message objects\n",
    "        user_question: The user's question about an issue (e.g., \"why did they choose this room\")\n",
    "        llm_model: Optional LLM model. If None, will try to use get_chat_model() or ChatOpenAI\n",
    "        max_messages_to_check: Maximum number of messages to check (starting from most recent)\n",
    "        confidence_threshold: Minimum confidence score (0-1) to consider a message relevant\n",
    "    \n",
    "    Returns:\n",
    "        List of tuples: (message_dict, confidence_score, explanation)\n",
    "        Sorted by confidence score (highest first)\n",
    "    \"\"\"\n",
    "    # Initialize LLM if not provided\n",
    "    if llm_model is None:\n",
    "        llm_model = get_chat_model(\"claude-3-5-sonnet\")\n",
    "        \n",
    "    \n",
    "    # Extract messages\n",
    "    if isinstance(trace_data, dict) and \"messages\" in trace_data:\n",
    "        # Handle LangChain message objects\n",
    "        messages = []\n",
    "        for i, msg in enumerate(trace_data[\"messages\"]):\n",
    "            if isinstance(msg, BaseMessage):\n",
    "                msg_dict = {\n",
    "                    \"message\": {\n",
    "                        \"type\": msg.__class__.__name__.replace(\"Message\", \"\").lower(),\n",
    "                        \"content\": msg.content if hasattr(msg, \"content\") else str(msg),\n",
    "                        \"tool_calls\": getattr(msg, \"tool_calls\", []),\n",
    "                        \"additional_kwargs\": getattr(msg, \"additional_kwargs\", {})\n",
    "                    },\n",
    "                    \"source\": \"output\",\n",
    "                    \"run_id\": f\"msg_{i}\",\n",
    "                    \"run_name\": \"trace\",\n",
    "                    \"run_type\": \"message\",\n",
    "                    \"timestamp\": None,\n",
    "                    \"index\": i\n",
    "                }\n",
    "                messages.append(msg_dict)\n",
    "    else:\n",
    "        # Handle trace data structure\n",
    "        messages = extract_messages_from_trace(trace_data)\n",
    "        # Add index for reverse ordering\n",
    "        for i, msg in enumerate(messages):\n",
    "            msg[\"index\"] = i\n",
    "    \n",
    "    if not messages:\n",
    "        return []\n",
    "    \n",
    "    # Sort by timestamp (most recent first) or by index (reverse)\n",
    "    messages_sorted = sorted(\n",
    "        messages,\n",
    "        key=lambda x: (x.get(\"timestamp\") or \"\"),\n",
    "        reverse=True\n",
    "    )[:max_messages_to_check]\n",
    "    \n",
    "    # If no timestamps, reverse by index\n",
    "    if not any(m.get(\"timestamp\") for m in messages_sorted):\n",
    "        messages_sorted = list(reversed(messages))[:max_messages_to_check]\n",
    "    \n",
    "    # Create evaluation prompt\n",
    "    evaluation_prompt = ChatPromptTemplate.from_messages([\n",
    "        (\"system\", \"\"\"You are an expert at analyzing agent execution traces to find the root cause of issues.\n",
    "\n",
    "Your task is to evaluate whether a specific message in an execution trace is the origin or cause of a user-reported issue.\n",
    "\n",
    "For each message, provide:\n",
    "1. A confidence score from 0.0 to 1.0 indicating how likely this message is the origin of the issue\n",
    "2. A brief explanation (1-2 sentences) of why this message is or isn't relevant\n",
    "\n",
    "Consider:\n",
    "- Does this message contain the decision, action, or information that led to the issue?\n",
    "- Is this message where the problematic choice was made?\n",
    "- Does this message show incorrect reasoning or data that caused the problem?\n",
    "\n",
    "Respond in JSON format:\n",
    "{\"confidence\": 0.0-1.0, \"explanation\": \"brief explanation\"}\"\"\"),\n",
    "        (\"human\", \"\"\"User Question: {user_question}\n",
    "\n",
    "Message to evaluate:\n",
    "{message_content}\n",
    "\n",
    "Context: This is message {message_index} of {total_messages} (evaluating in reverse chronological order, most recent first).\n",
    "\n",
    "Provide your evaluation as JSON with 'confidence' and 'explanation' fields.\"\"\")\n",
    "    ])\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    # Evaluate each message\n",
    "    for idx, msg_dict in enumerate(messages_sorted):\n",
    "        message_content = format_message_for_analysis(msg_dict)\n",
    "        \n",
    "        try:\n",
    "            # Get LLM evaluation\n",
    "            chain = evaluation_prompt | llm_model\n",
    "            response = chain.invoke({\n",
    "                \"user_question\": user_question,\n",
    "                \"message_content\": message_content,\n",
    "                \"message_index\": idx + 1,\n",
    "                \"total_messages\": len(messages_sorted)\n",
    "            })\n",
    "            \n",
    "            # Parse response\n",
    "            response_text = response.content if hasattr(response, \"content\") else str(response)\n",
    "            \n",
    "            # Try to extract JSON from response\n",
    "            import json\n",
    "            import re\n",
    "            \n",
    "            # Look for JSON in the response\n",
    "            json_match = re.search(r'\\{[^{}]*\"confidence\"[^{}]*\\}', response_text, re.DOTALL)\n",
    "            if json_match:\n",
    "                try:\n",
    "                    eval_result = json.loads(json_match.group())\n",
    "                    confidence = float(eval_result.get(\"confidence\", 0.0))\n",
    "                    explanation = eval_result.get(\"explanation\", \"No explanation provided\")\n",
    "                except:\n",
    "                    # Fallback: try to extract confidence from text\n",
    "                    conf_match = re.search(r'confidence[\"\\s:]+([0-9.]+)', response_text, re.IGNORECASE)\n",
    "                    confidence = float(conf_match.group(1)) if conf_match else 0.0\n",
    "                    explanation = response_text[:200]\n",
    "            else:\n",
    "                # Fallback parsing\n",
    "                conf_match = re.search(r'confidence[\"\\s:]+([0-9.]+)', response_text, re.IGNORECASE)\n",
    "                confidence = float(conf_match.group(1)) if conf_match else 0.0\n",
    "                explanation = response_text[:200]\n",
    "            \n",
    "            if confidence >= confidence_threshold:\n",
    "                results.append((msg_dict, confidence, explanation))\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Error evaluating message {idx + 1}: {e}\")\n",
    "            continue\n",
    "    \n",
    "    # Sort by confidence (highest first)\n",
    "    results.sort(key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    return results\n",
    "\n",
    "\n",
    "def print_issue_analysis(\n",
    "    trace_data: List[Dict] | Dict[str, Any],\n",
    "    user_question: str,\n",
    "    llm_model: Optional[Any] = None,\n",
    "    max_results: int = 5\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Convenience function to find and print issue origins.\n",
    "    \"\"\"\n",
    "    results = find_issue_origin(trace_data, user_question, llm_model)\n",
    "    \n",
    "    if not results:\n",
    "        print(f\"No messages found that are likely origins of: '{user_question}'\")\n",
    "        return\n",
    "    \n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"ISSUE ANALYSIS: {user_question}\")\n",
    "    print(f\"{'='*80}\\n\")\n",
    "    print(f\"Found {len(results)} relevant message(s):\\n\")\n",
    "    \n",
    "    for i, (msg_dict, confidence, explanation) in enumerate(results[:max_results], 1):\n",
    "        msg = msg_dict[\"message\"]\n",
    "        msg_type = msg.get(\"type\", \"unknown\")\n",
    "        content = msg.get(\"content\", \"\")[:150]\n",
    "        \n",
    "        print(f\"{i}. [Confidence: {confidence:.2f}]\")\n",
    "        print(f\"   Type: {msg_type.upper()}\")\n",
    "        print(f\"   Source: {msg_dict['source']} from {msg_dict.get('run_name', 'unknown')}\")\n",
    "        print(f\"   Content: {content}...\")\n",
    "        print(f\"   Explanation: {explanation}\")\n",
    "        print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "22a7c876",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "No LLM model available. Please provide one or set up API keys.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mOpenAIError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[55]\u001b[39m\u001b[32m, line 160\u001b[39m, in \u001b[36mfind_issue_origin\u001b[39m\u001b[34m(trace_data, user_question, llm_model, max_messages_to_check, confidence_threshold)\u001b[39m\n\u001b[32m    159\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m160\u001b[39m     llm_model = \u001b[43mChatOpenAI\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mgpt-4\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    161\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/genjitsu/lib/python3.12/site-packages/langchain_core/load/serializable.py:116\u001b[39m, in \u001b[36mSerializable.__init__\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    115\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\"\"\"\u001b[39;00m  \u001b[38;5;66;03m# noqa: D419  # Intentional blank docstring\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m116\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "    \u001b[31m[... skipping hidden 1 frame]\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/genjitsu/lib/python3.12/site-packages/langchain_openai/chat_models/base.py:948\u001b[39m, in \u001b[36mBaseChatOpenAI.validate_environment\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    941\u001b[39m async_specific = {\n\u001b[32m    942\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mhttp_client\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m.http_async_client\n\u001b[32m    943\u001b[39m     \u001b[38;5;129;01mor\u001b[39;00m _get_default_async_httpx_client(\n\u001b[32m   (...)\u001b[39m\u001b[32m    946\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mapi_key\u001b[39m\u001b[33m\"\u001b[39m: async_api_key_value,\n\u001b[32m    947\u001b[39m }\n\u001b[32m--> \u001b[39m\u001b[32m948\u001b[39m \u001b[38;5;28mself\u001b[39m.root_async_client = \u001b[43mopenai\u001b[49m\u001b[43m.\u001b[49m\u001b[43mAsyncOpenAI\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    949\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mclient_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    950\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43masync_specific\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[arg-type]\u001b[39;49;00m\n\u001b[32m    951\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    952\u001b[39m \u001b[38;5;28mself\u001b[39m.async_client = \u001b[38;5;28mself\u001b[39m.root_async_client.chat.completions\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/genjitsu/lib/python3.12/site-packages/openai/_client.py:488\u001b[39m, in \u001b[36mAsyncOpenAI.__init__\u001b[39m\u001b[34m(self, api_key, organization, project, webhook_secret, base_url, websocket_base_url, timeout, max_retries, default_headers, default_query, http_client, _strict_response_validation)\u001b[39m\n\u001b[32m    487\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m api_key \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m488\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m OpenAIError(\n\u001b[32m    489\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mThe api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    490\u001b[39m     )\n\u001b[32m    491\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcallable\u001b[39m(api_key):\n",
      "\u001b[31mOpenAIError\u001b[39m: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[56]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Example usage with the example_output from Cell 1\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;66;03m# Analyze why the agent chose the community center\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m \u001b[43mprint_issue_analysis\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      5\u001b[39m \u001b[43m    \u001b[49m\u001b[43mexample_output\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      6\u001b[39m \u001b[43m    \u001b[49m\u001b[43muser_question\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mwhy did they choose this room\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      7\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmax_results\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m3\u001b[39;49m\n\u001b[32m      8\u001b[39m \u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[55]\u001b[39m\u001b[32m, line 296\u001b[39m, in \u001b[36mprint_issue_analysis\u001b[39m\u001b[34m(trace_data, user_question, llm_model, max_results)\u001b[39m\n\u001b[32m    287\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mprint_issue_analysis\u001b[39m(\n\u001b[32m    288\u001b[39m     trace_data: List[Dict] | Dict[\u001b[38;5;28mstr\u001b[39m, Any],\n\u001b[32m    289\u001b[39m     user_question: \u001b[38;5;28mstr\u001b[39m,\n\u001b[32m    290\u001b[39m     llm_model: Optional[Any] = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m    291\u001b[39m     max_results: \u001b[38;5;28mint\u001b[39m = \u001b[32m5\u001b[39m\n\u001b[32m    292\u001b[39m ) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    293\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    294\u001b[39m \u001b[33;03m    Convenience function to find and print issue origins.\u001b[39;00m\n\u001b[32m    295\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m296\u001b[39m     results = \u001b[43mfind_issue_origin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrace_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muser_question\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mllm_model\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    298\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m results:\n\u001b[32m    299\u001b[39m         \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mNo messages found that are likely origins of: \u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00muser_question\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[55]\u001b[39m\u001b[32m, line 162\u001b[39m, in \u001b[36mfind_issue_origin\u001b[39m\u001b[34m(trace_data, user_question, llm_model, max_messages_to_check, confidence_threshold)\u001b[39m\n\u001b[32m    160\u001b[39m             llm_model = ChatOpenAI(model=\u001b[33m\"\u001b[39m\u001b[33mgpt-4\u001b[39m\u001b[33m\"\u001b[39m, temperature=\u001b[32m0\u001b[39m)\n\u001b[32m    161\u001b[39m         \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m162\u001b[39m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mNo LLM model available. Please provide one or set up API keys.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    164\u001b[39m \u001b[38;5;66;03m# Extract messages\u001b[39;00m\n\u001b[32m    165\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(trace_data, \u001b[38;5;28mdict\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mmessages\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m trace_data:\n\u001b[32m    166\u001b[39m     \u001b[38;5;66;03m# Handle LangChain message objects\u001b[39;00m\n",
      "\u001b[31mValueError\u001b[39m: No LLM model available. Please provide one or set up API keys."
     ]
    }
   ],
   "source": [
    "# Example usage with the example_output from Cell 1\n",
    "# Analyze why the agent chose the community center\n",
    "\n",
    "print_issue_analysis(\n",
    "    example_output,\n",
    "    user_question=\"why did they choose this room\",\n",
    "    max_results=3\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fb5c7a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[36m[HumanMessage] \u001b[0mHuman: I would like to book the community center for a 30-person event on December 10th.\n",
      "\u001b[1m\u001b[32m[AIMessage   ] \u001b[0mTool call: check_calendar({'date': '2023-12-10'})\n",
      "\u001b[1m\u001b[35m[ToolMessage ] \u001b[0mObservation:2023-12-10 is AVAILABLE\n",
      "\u001b[1m\u001b[32m[AIMessage   ] \u001b[0mTool call: check_room_rules({'room_id': 'community_center'})\n",
      "\u001b[1m\u001b[35m[ToolMessage ] \u001b[0mObservation: Rules for community_center are 'max_capacity: 50, no_food_allowed'.\n",
      "\u001b[1m\u001b[32m[AIMessage   ] \u001b[0mTool call: assign_task({'staff_name': 'John', 'task': 'Prepare community center for December 10th event - 30 people'})\n",
      "\u001b[1m\u001b[35m[ToolMessage ] \u001b[0massign_task: Action successful: Task 'Prepare community center for December 10th event - 30 people' assigned to J\n",
      "\u001b[1m\u001b[32m[AIMessage   ] \u001b[0mTool call: final_answer({'response': \"Your booking for the community center on December 10th has been confirmed. The venue can accommodate your 30-person event as it's within our maximum capacity of 50 people. Please note that no food is allowed in the community center. I've assigned staff to prepare the venue for your event.\"})\n",
      "\u001b[1m\u001b[35m[ToolMessage ] \u001b[0mfinal_answer: Final Response to User: Your booking for the community center on December 10th has been confirmed. T\n",
      "\u001b[1m\u001b[32m[AIMessage   ] \u001b[0m{'content': [], 'usage': {'input_tokens': 1781, 'output_tokens': 3, 'total_tokens': 1784}, 'metadata\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.messages import HumanMessage, AIMessage, ToolMessage\n",
    "\n",
    "example_output = {\n",
    "    'messages': [\n",
    "        HumanMessage(\n",
    "            content='\\nYou are a helpful Town Hall agent. Your goal is to process requests by thinking step-by-step and using the available tools. You must use the check_calendar, check_room_rules, assign_task, and final_answer tools. You will follow this pattern:\\n\\nHuman: [The user\\'s request]\\nAgent thought: [Your reasoning and plan]\\nAgent action: [The tool you will use]\\nObservation: [The result from the tool]\\n... (repeat thought/action/observation) ...\\nAgent thought: [Your final reasoning]\\nAgent action: final_answer(\"[Your response to the human]\")\\n\\n---\\nHere are some examples:\\n---\\n\\nHuman: I need to book the community center for a 20-person meeting on Dec 5th.\\nAgent thought: The user wants to book \\'community_center\\' on \\'Dec 5th\\' for 20 people. First, I must check if the room is available on that date.\\nAgent action: check_calendar(date=\"2025-12-05\")\\nObservation: The calendar shows this date is AVAILABLE.\\nAgent thought: The date is available. Now I need to check the rules for the \\'community_center\\' to ensure a 20-person meeting is allowed.\\nAgent action: check_room_rules(room_id=\"community_center\")\\nObservation: Rules for community_center are \\'max_capacity: 50, no_food_allowed\\'.\\nAgent thought: The 20-person meeting is within the 50-person capacity. The request is valid. I can now book it and inform the user of the \\'no_food_allowed\\' rule.\\nAgent action: final_answer(\"Booking confirmed for the community center on Dec 5th. Please note: no food is allowed.\")\\n\\n---\\nHuman: Please assign the \\'Setup chairs\\' ticket to Bob.\\nAgent thought: I need to assign the task \\'Setup chairs\\' to \\'Bob\\'. I must use the assign_task tool.\\nAgent action: assign_task(staff_name=\"Bob\", task=\"Setup chairs\")\\nObservation: Action was not successful. Reason: \\'Bob\\' is marked as \\'On-Leave\\'.\\nAgent thought: I cannot assign the task to Bob because he is on leave. This is a failure[cite: 173]. I must inform the user that I cannot complete their request and state the reason.\\nAgent action: final_answer(\"I\\'m sorry, I cannot assign \\'Setup chairs\\' to Bob. My system shows he is \\'On-Leave\\'.\")\\nHuman: I would like to book the community center for a 30-person event on December 10th.', \n",
    "            additional_kwargs={}, response_metadata={}, id='2aca527f-36ec-4d17-a6a7-8daa84f44a9c'), \n",
    "        AIMessage(content='', additional_kwargs={}, response_metadata={}, \n",
    "            id='lc_run--0482436b-58f9-49e4-9fa7-b5825c1a51dd-0', \n",
    "            tool_calls=[{'name': 'check_calendar', 'args': {'date': '2023-12-10'}, \n",
    "            'id': 'toolu_bdrk_01726hSPHm9cVQ17LqHCqfPp', 'type': 'tool_call'}]), \n",
    "        ToolMessage(content='Observation:2023-12-10 is AVAILABLE', name='check_calendar', \n",
    "                    id='9e2ab60c-da09-4f96-a6a1-fe0efebe7ba4', tool_call_id='toolu_bdrk_01726hSPHm9cVQ17LqHCqfPp'), \n",
    "        AIMessage(content='', additional_kwargs={}, response_metadata={}, \n",
    "            id='lc_run--952a9fb0-8a9a-4544-8b33-f93c2d21a823-0', \n",
    "            tool_calls=[{'name': 'check_room_rules', 'args': {'room_id': 'community_center'}, \n",
    "                        'id': 'toolu_bdrk_01LYBoqJQsxMGycuDsFSRuLZ', 'type': 'tool_call'}]), \n",
    "        ToolMessage(content=\"Observation: Rules for community_center are 'max_capacity: 50, no_food_allowed'.\", \n",
    "                    name='check_room_rules', id='b2c5834c-6ff8-4d57-a912-73267d178425', \n",
    "                    tool_call_id='toolu_bdrk_01LYBoqJQsxMGycuDsFSRuLZ'), \n",
    "        AIMessage(content='', additional_kwargs={}, response_metadata={}, \n",
    "                    id='lc_run--e177284f-30c5-4e8a-8700-e9f9e11556a9-0', \n",
    "                    tool_calls=[{'name': 'assign_task', 'args': {'staff_name': 'John', 'task': 'Prepare community center for December 10th event - 30 people'}, \n",
    "                    'id': 'toolu_bdrk_01QrZTgBShYFdnVtJRQcqYqY', 'type': 'tool_call'}]), \n",
    "        ToolMessage(\n",
    "            content=\"Action successful: Task 'Prepare community center for December 10th event - 30 people' assigned to John.\", name='assign_task', id='8cba59a0-fb01-4c23-ae81-7200bc3b4ac3', tool_call_id='toolu_bdrk_01QrZTgBShYFdnVtJRQcqYqY'), \n",
    "        AIMessage(content='', additional_kwargs={}, response_metadata={}, \n",
    "            id='lc_run--4974f8ea-df0f-44f4-a97e-9faf1150adb3-0', \n",
    "            tool_calls=[{'name': 'final_answer', 'args': {'response': \"Your booking for the community center on December 10th has been confirmed. The venue can accommodate your 30-person event as it's within our maximum capacity of 50 people. Please note that no food is allowed in the community center. I've assigned staff to prepare the venue for your event.\"}, 'id': 'toolu_bdrk_01DpnTN47Q2e45qptFzfhtgp', 'type': 'tool_call'}]), \n",
    "        ToolMessage(content=\"Final Response to User: Your booking for the community center on December 10th has been confirmed. The venue can accommodate your 30-person event as it's within our maximum capacity of 50 people. Please note that no food is allowed in the community center. I've assigned staff to prepare the venue for your event.\", \n",
    "            name='final_answer', \n",
    "            id='754dab08-0270-4b95-91a2-e5e796dad0a1', \n",
    "            tool_call_id='toolu_bdrk_01DpnTN47Q2e45qptFzfhtgp'), \n",
    "        AIMessage(\n",
    "            content=\"{'content': [], 'usage': {'input_tokens': 1781, 'output_tokens': 3, 'total_tokens': 1784}, 'metadata': {'team_id': 'team_the_great_hack_2025_046', 'model': 'us.anthropic.claude-3-5-sonnet-20241022-v2:0', 'cost_usd': 0.005388, 'latency_ms': 906.67, 'remaining_quota': {'requests_today': 35, 'tokens_today': 186752, 'llm_cost': 0.628632, 'gpu_cost': 0.0, 'total_cost': 0.628632, 'budget_limit': 50.0, 'remaining_budget': 49.371368, 'budget_usage_percent': 1.257264}}}\", \n",
    "            additional_kwargs={}, \n",
    "            response_metadata={}, \n",
    "            id='lc_run--ec5829ea-602c-4dbc-b855-3cdba4340b46-0')\n",
    "        ], \n",
    "    # 'structured_response': TechAnalysis(\n",
    "    #     thought=\"I'll format this API response data as a structured JSON object. The data contains usage statistics, metadata about the team, model, costs, latency, and quota information. I'll present it in a properly indented, readable JSON format.\", \n",
    "    #     action='{\\n  \"content\": [],\\n  \"usage\": {\\n    \"input_tokens\": 1781,\\n    \"output_tokens\": 3,\\n    \"total_tokens\": 1784\\n  },\\n  \"metadata\": {\\n    \"team_id\": \"team_the_great_hack_2025_046\",\\n    \"model\": \"us.anthropic.claude-3-5-sonnet-20241022-v2:0\",\\n    \"cost_usd\": 0.005388,\\n    \"latency_ms\": 906.67,\\n    \"remaining_quota\": {\\n      \"requests_today\": 35,\\n      \"tokens_today\": 186752,\\n      \"llm_cost\": 0.628632,\\n      \"gpu_cost\": 0.0,\\n      \"total_cost\": 0.628632,\\n      \"budget_limit\": 50.0,\\n      \"remaining_budget\": 49.371368,\\n      \"budget_usage_percent\": 1.257264\\n    }\\n  }\\n}')\n",
    "    }\n",
    "from termcolor import colored\n",
    "\n",
    "def print_message_summary(msg):\n",
    "    \"\"\"\n",
    "    Pretty-print a summary of a message object with colored type and content/redirect.\n",
    "    Handles HumanMessage, AIMessage, ToolMessage.\n",
    "    \"\"\"\n",
    "    msg_type = type(msg).__name__\n",
    "    content = getattr(msg, 'content', '')\n",
    "    if msg_type == \"HumanMessage\":\n",
    "        tcolor = 'cyan'\n",
    "        important = content.strip().split(\"\\n\")[-1][:100]  # Last line of prompt, max 100 chars\n",
    "    elif msg_type == \"AIMessage\":\n",
    "        tcolor = 'green'\n",
    "        # If tool call(s), summarize; else, show content\n",
    "        tool_calls = getattr(msg, 'tool_calls', None)\n",
    "        if tool_calls and len(tool_calls) > 0:\n",
    "            tc = tool_calls[0]\n",
    "            important = f\"Tool call: {tc.get('name', '')}({tc.get('args', {})})\"\n",
    "        else:\n",
    "            important = content.strip()[:100]\n",
    "    elif msg_type == \"ToolMessage\":\n",
    "        tcolor = 'magenta'\n",
    "        toolname = getattr(msg, \"name\", \"\")\n",
    "        if content.startswith(\"Observation\"):\n",
    "            important = content.strip()[:120]\n",
    "        else:\n",
    "            important = f\"{toolname}: {content.strip()[:100]}\"\n",
    "    else:\n",
    "        tcolor='yellow'\n",
    "        important = str(content)[:100]\n",
    "    print(colored(f\"[{msg_type:12s}] \", tcolor, attrs=[\"bold\"]), end='')\n",
    "    print(important)\n",
    "\n",
    "# Example usage:\n",
    "for m in example_output['messages']:\n",
    "    print_message_summary(m)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3490f8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "b418220f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Visualizing /home/dennisushi/personal/hackthon-2025/track_b_glass_box/traces/trace-very-simple.json\n",
      "> input: {'messages': [{'type': 'human', 'content': 'What is a GPU?\\n'}]}\n",
      "- ReAct Agent (chain)\n",
      "  ↳ input: {'messages': [{'type': 'human', 'content': 'What is a GPU?\\n'}]}\n",
      "  ↳ output: {'messages': [{'content': 'What is a GPU?\\n', 'additional_kwargs': {}, 'response_metadata': {}, 'type': 'human', 'id': '\n",
      "  - call_model (chain)\n",
      "    ↳ input: {'input': {'messages': [{'content': 'What is a GPU?\\n', 'additional_kwargs': {}, 'response_metadata': {}, 'type': 'human\n",
      "    ↳ output: {'messages': [{'content': 'A GPU, or Graphics Processing Unit, is a specialized electronic circuit designed to accelerat\n",
      "    - ChatOpenAI (llm)\n",
      "      ↳ input: {'messages': [[{'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'messages', 'SystemMessage'], 'kwargs': {'\n",
      "      ↳ output: {'generations': [{'text': 'A GPU, or Graphics Processing Unit, is a specialized electronic circuit designed to accelerat\n",
      "    - route_model_output (chain)\n",
      "      ↳ input: {'input': {'messages': [{'content': 'What is a GPU?\\n', 'additional_kwargs': {}, 'response_metadata': {}, 'type': 'human\n",
      "      ↳ output: {'output': '__end__'}\n",
      "> output: {'output': '__end__'}\n",
      "Loaded trace with 0 runs\n",
      "Created 0 nodes and 0 edges\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Default: visualize the very simple trace (Jupyter-friendly)\n",
    "import os\n",
    "\n",
    "try:\n",
    "    # Try to use current notebook working directory\n",
    "    current_dir = Path().resolve()\n",
    "    trace_dir = current_dir.parent.parent / \"track_b_glass_box\" / \"traces\"\n",
    "except Exception as e:\n",
    "    print(\"Could not determine path to traces directory:\", e)\n",
    "    trace_dir = Path(\".\")\n",
    "trace_file = trace_dir / \"trace-very-simple.json\"\n",
    "\n",
    "if trace_file.exists():\n",
    "    print(f\"Visualizing {trace_file}\")\n",
    "    visualize_trace(trace_file, interactive=True)\n",
    "else:\n",
    "    print(f\"Trace file {trace_file} does not exist\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f42456e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "from typing import List, Dict, Any\n",
    "\n",
    "\n",
    "def build_interaction_tree(trace_data: List[Dict]) -> Dict[str, Any]:\n",
    "    \"\"\"Build a hierarchical interaction tree with actor labels (human, ai, tool, agent).\"\"\"\n",
    "\n",
    "    # Build graph relationships\n",
    "    graph = defaultdict(list)\n",
    "    id_to_run = {r[\"id\"]: r for r in trace_data}\n",
    "    for r in trace_data:\n",
    "        for parent in r.get(\"parent_run_ids\", []):\n",
    "            graph[parent].append(r[\"id\"])\n",
    "\n",
    "    def classify_actor(run: Dict) -> str:\n",
    "        \"\"\"Return 'human', 'ai', 'tool', or 'agent'.\"\"\"\n",
    "        rt = run.get(\"run_type\", \"\")\n",
    "        name = run.get(\"name\", \"\")\n",
    "\n",
    "        # Human message is always inside inputs/messages\n",
    "        if run.get(\"inputs\"):\n",
    "            msgs = run[\"inputs\"].get(\"messages\", [])\n",
    "            if msgs:\n",
    "                # Handle nested list structure: [[{...}]] or [{...}]\n",
    "                first_msg = msgs[0]\n",
    "                # If first element is a list, unwrap it\n",
    "                if isinstance(first_msg, list) and len(first_msg) > 0:\n",
    "                    first_msg = first_msg[0]\n",
    "                # Now check if it's a dict with type \"human\"\n",
    "                if isinstance(first_msg, dict) and first_msg.get(\"type\") == \"human\":\n",
    "                    return \"human\"\n",
    "\n",
    "        # LLM model\n",
    "        if rt == \"llm\" or \"ChatOpenAI\" in name or \"OpenAI\" in name:\n",
    "            return \"ai\"\n",
    "\n",
    "        # Tool call\n",
    "        if rt == \"tool\":\n",
    "            return \"tool\"\n",
    "\n",
    "        # Everything else is orchestrator\n",
    "        return \"agent\"\n",
    "\n",
    "    def short(obj):\n",
    "        if not obj:\n",
    "            return None\n",
    "        s = json.dumps(obj)\n",
    "        return s[:200] + (\"...\" if len(s) > 200 else \"\")\n",
    "\n",
    "    def build_node(run_id: str, depth=0):\n",
    "        run = id_to_run[run_id]\n",
    "        node = {\n",
    "            \"id\": run_id,\n",
    "            \"name\": run.get(\"name\"),\n",
    "            \"type\": run.get(\"run_type\"),\n",
    "            \"actor\": classify_actor(run),     # ← KEY LINE\n",
    "            \"depth\": depth,\n",
    "            \"input\": short(run.get(\"inputs\")),\n",
    "            \"output\": short(run.get(\"outputs\")),\n",
    "            \"children\": []\n",
    "        }\n",
    "        for child_id in graph.get(run_id, []):\n",
    "            node[\"children\"].append(build_node(child_id, depth + 1))\n",
    "        return node\n",
    "\n",
    "    # Find roots (no parent)\n",
    "    roots = [r[\"id\"] for r in trace_data if not r.get(\"parent_run_ids\")]\n",
    "    if not roots:\n",
    "        raise ValueError(\"No root nodes found in trace.\")\n",
    "\n",
    "    # If multiple roots, wrap them; otherwise return the only root\n",
    "    return (\n",
    "        build_node(roots[0])\n",
    "        if len(roots) == 1\n",
    "        else {\"roots\": [build_node(r) for r in roots]}\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "71688cd4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': '1f06b75b-3a54-6918-b0a3-d23cf007ec77',\n",
       " 'name': 'ReAct Agent',\n",
       " 'type': 'chain',\n",
       " 'actor': 'human',\n",
       " 'depth': 0,\n",
       " 'input': '{\"messages\": [{\"type\": \"human\", \"content\": \"What is a GPU?\\\\n\"}]}',\n",
       " 'output': '{\"messages\": [{\"content\": \"What is a GPU?\\\\n\", \"additional_kwargs\": {}, \"response_metadata\": {}, \"type\": \"human\", \"id\": \"e7849012-9dc8-4cb5-ab9a-7d3413943077\", \"example\": false}, {\"content\": \"A GPU, or...',\n",
       " 'children': [{'id': 'dcd21300-e41c-43f1-930c-88e707a5f13c',\n",
       "   'name': 'call_model',\n",
       "   'type': 'chain',\n",
       "   'actor': 'agent',\n",
       "   'depth': 1,\n",
       "   'input': '{\"input\": {\"messages\": [{\"content\": \"What is a GPU?\\\\n\", \"additional_kwargs\": {}, \"response_metadata\": {}, \"type\": \"human\", \"id\": \"e7849012-9dc8-4cb5-ab9a-7d3413943077\", \"example\": false}], \"is_last_st...',\n",
       "   'output': '{\"messages\": [{\"content\": \"A GPU, or Graphics Processing Unit, is a specialized electronic circuit designed to accelerate the processing of images and graphics. It is highly efficient at performing pa...',\n",
       "   'children': [{'id': 'f9d5b500-69d0-4751-a9d6-a78df153d3be',\n",
       "     'name': 'ChatOpenAI',\n",
       "     'type': 'llm',\n",
       "     'actor': 'ai',\n",
       "     'depth': 2,\n",
       "     'input': '{\"messages\": [[{\"lc\": 1, \"type\": \"constructor\", \"id\": [\"langchain\", \"schema\", \"messages\", \"SystemMessage\"], \"kwargs\": {\"content\": \"You are a helpful AI assistant.\\\\n\\\\nSystem time: 2025-07-28T05:42:53.8...',\n",
       "     'output': '{\"generations\": [{\"text\": \"A GPU, or Graphics Processing Unit, is a specialized electronic circuit designed to accelerate the processing of images and graphics. It is highly efficient at performing pa...',\n",
       "     'children': []},\n",
       "    {'id': 'f7daeb33-7155-419e-ab55-a5f59006c830',\n",
       "     'name': 'route_model_output',\n",
       "     'type': 'chain',\n",
       "     'actor': 'agent',\n",
       "     'depth': 2,\n",
       "     'input': '{\"input\": {\"messages\": [{\"content\": \"What is a GPU?\\\\n\", \"additional_kwargs\": {}, \"response_metadata\": {}, \"type\": \"human\", \"id\": \"e7849012-9dc8-4cb5-ab9a-7d3413943077\", \"example\": false}, {\"content\": ...',\n",
       "     'output': '{\"output\": \"__end__\"}',\n",
       "     'children': []}]},\n",
       "  {'id': 'f9d5b500-69d0-4751-a9d6-a78df153d3be',\n",
       "   'name': 'ChatOpenAI',\n",
       "   'type': 'llm',\n",
       "   'actor': 'ai',\n",
       "   'depth': 1,\n",
       "   'input': '{\"messages\": [[{\"lc\": 1, \"type\": \"constructor\", \"id\": [\"langchain\", \"schema\", \"messages\", \"SystemMessage\"], \"kwargs\": {\"content\": \"You are a helpful AI assistant.\\\\n\\\\nSystem time: 2025-07-28T05:42:53.8...',\n",
       "   'output': '{\"generations\": [{\"text\": \"A GPU, or Graphics Processing Unit, is a specialized electronic circuit designed to accelerate the processing of images and graphics. It is highly efficient at performing pa...',\n",
       "   'children': []},\n",
       "  {'id': 'f7daeb33-7155-419e-ab55-a5f59006c830',\n",
       "   'name': 'route_model_output',\n",
       "   'type': 'chain',\n",
       "   'actor': 'agent',\n",
       "   'depth': 1,\n",
       "   'input': '{\"input\": {\"messages\": [{\"content\": \"What is a GPU?\\\\n\", \"additional_kwargs\": {}, \"response_metadata\": {}, \"type\": \"human\", \"id\": \"e7849012-9dc8-4cb5-ab9a-7d3413943077\", \"example\": false}, {\"content\": ...',\n",
       "   'output': '{\"output\": \"__end__\"}',\n",
       "   'children': []}]}"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "include_messages = True\n",
    "trace_path = trace_file\n",
    "builder = TraceGraphBuilder(include_messages=include_messages)\n",
    "trace_data = builder.load_trace(trace_path)\n",
    "build_interaction_tree(trace_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2934c9ce",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "genjitsu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
